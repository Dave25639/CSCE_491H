{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alive_progress import alive_bar\n",
    "import requests\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from io import StringIO\n",
    "import json\n",
    "import os\n",
    "import zipfile\n",
    "import tarfile\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm.asyncio import tqdm as tqdm_async\n",
    "import aiohttp\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: see if possible to implement async I/O and just have a single session open instead of creating a million requests \n",
    "\n",
    "class GDCDataFetcher:\n",
    "    GDC_BASE_URL = \"https://api.gdc.cancer.gov\"\n",
    "    \n",
    "    def __init__(self, project_id=None):\n",
    "        self.project_id = project_id\n",
    "\n",
    "    def check_status(self):\n",
    "        \"\"\"Check the status of the GDC API.\"\"\"\n",
    "        url = f\"{self.GDC_BASE_URL}/status\"\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                return response.json()\n",
    "            else:\n",
    "                print(f\"Error: Unable to retrieve status. HTTP Status Code: {response.status_code}\")\n",
    "                return None\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def display_available_categories_and_type(self):\n",
    "        \"\"\"Get all data_categories and data_types available for the project.\"\"\"\n",
    "        url = f\"{self.GDC_BASE_URL}/files\"\n",
    "\n",
    "        params = {\n",
    "            \"filters\": {\n",
    "                \"op\": \"in\",\n",
    "                \"content\": {\n",
    "                    \"field\": \"cases.project.project_id\",\n",
    "                    \"value\": [self.project_id]\n",
    "                }\n",
    "            },\n",
    "            \"fields\": \"file_id,file_name,data_category,data_type\",\n",
    "            \"format\": \"JSON\",\n",
    "            \"size\": \"10000\" # returns min(size, available_files)\n",
    "        }\n",
    "\n",
    "        response = requests.post(url, json=params)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()['data']['hits']\n",
    "            df = pd.DataFrame(data)\n",
    "            # display unique data_category and data_type combinations\n",
    "            print(df[['data_category', 'data_type']].drop_duplicates())\n",
    "        else:\n",
    "            print(f\"Error fetching data: {response.status_code}\")\n",
    "\n",
    "    def fetch_cases(self, case_ids=None, fields=None):\n",
    "        \"\"\"\n",
    "        Fetch case data for specified case IDs or all cases in the project, \n",
    "        and return as a Pandas DataFrame.\n",
    "        \n",
    "        :param case_ids: Optional list of case IDs to filter the results.\n",
    "        :param fields: Optional list of fields to retrieve.\n",
    "        \n",
    "        :return: A Pandas DataFrame containing the case data if successful, None otherwise.\n",
    "        \"\"\"\n",
    "        url = f\"{self.GDC_BASE_URL}/cases\"\n",
    "        filters = {\n",
    "            \"op\": \"and\",\n",
    "            \"content\": []\n",
    "        }\n",
    "\n",
    "        if case_ids:\n",
    "            filters[\"content\"].append({\n",
    "                \"op\": \"in\",\n",
    "                \"content\": {\n",
    "                    \"field\": \"case_id\",\n",
    "                    \"value\": case_ids\n",
    "                }\n",
    "            })\n",
    "        \n",
    "        if self.project_id:\n",
    "            filters[\"content\"].append({\n",
    "                \"op\": \"in\",\n",
    "                \"content\": {\n",
    "                    \"field\": \"project.project_id\",\n",
    "                    \"value\": [self.project_id]\n",
    "                }\n",
    "            })\n",
    "\n",
    "        if not fields:\n",
    "            fields = [\"submitter_id\"]\n",
    "        \n",
    "        fields_param = ','.join(fields)\n",
    "\n",
    "        params = {\n",
    "            \"filters\": json.dumps(filters),\n",
    "            \"fields\": fields_param,\n",
    "            \"format\": \"JSON\",\n",
    "            \"size\": \"10000\"\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = requests.post(url, json=params)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()[\"data\"][\"hits\"]\n",
    "                df = pd.DataFrame(data)\n",
    "                return df\n",
    "            else:\n",
    "                print(f\"Error fetching cases: {response.status_code}\")\n",
    "                print(\"Response content:\", response.text)\n",
    "                return None\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"An error occurred during the fetch cases request: {e}\")\n",
    "            return None\n",
    "\n",
    "    def fetch_file_links(self, case_ids, fields=None, data_categories=None):\n",
    "        \"\"\"\n",
    "        Fetch metadata and files (e.g., images, genomic data) associated with the given case IDs from the GDC API, \n",
    "        and return as a Pandas DataFrame. \n",
    "        \n",
    "        :param case_ids: Required list of case IDs.\n",
    "        :param fields: Optional list of fields to retrieve for each file. If not specified, defaults to \n",
    "            retrieving 'file_id', 'file_name', 'case_id', 'data_type', and 'data_category'.\n",
    "        :param data_types: Optional list of data types to filter the files (e.g., \"SVS\", \"Gene Expression Quantification\"). \n",
    "                If not specified, retrieves files of all data types.\n",
    "        \n",
    "        :return: A Pandas DataFrame containing metadata for the retrieved files. Returns None if no files are found or if an error occurs.\n",
    "        \"\"\"\n",
    "        url = f\"{self.GDC_BASE_URL}/files\"\n",
    "        filters = {\n",
    "            \"op\": \"and\",\n",
    "            \"content\":[\n",
    "                {\n",
    "                \"op\": \"in\",\n",
    "                \"content\": {\n",
    "                    \"field\": \"cases.case_id\",\n",
    "                    \"value\": case_ids\n",
    "                }\n",
    "                },\n",
    "                {\n",
    "                \"op\": \"=\",\n",
    "                \"content\": {\n",
    "                    \"field\": \"access\",\n",
    "                    \"value\": \"open\"\n",
    "                }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        if data_categories:\n",
    "            filters[\"content\"].append({\n",
    "                \"op\": \"in\",\n",
    "                \"content\": {\n",
    "                    \"field\": \"data_category\",\n",
    "                    \"value\": data_categories\n",
    "                }\n",
    "            })\n",
    "\n",
    "        if not fields:\n",
    "            fields = [\"file_id\", \"file_name\", \"cases.case_id\", \"data_category\", \"data_type\", \"file_size\"]\n",
    "            \n",
    "        fields_param = ','.join(fields)\n",
    "\n",
    "        params = {\n",
    "            \"filters\": json.dumps(filters),\n",
    "            \"fields\": fields_param,\n",
    "            \"format\": \"JSON\",\n",
    "            \"size\": \"1000000\"\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = requests.post(url, json=params)\n",
    "            if response.status_code == 200:\n",
    "                file_metadata = response.json()[\"data\"][\"hits\"]\n",
    "                if not file_metadata:\n",
    "                    print(\"No files found for the provided case IDs.\")\n",
    "                    return None\n",
    "\n",
    "                df = pd.DataFrame(file_metadata)\n",
    "                \n",
    "                return df\n",
    "            else:\n",
    "                print(f\"Error fetching file links: {response.status_code}\")\n",
    "                print(\"Response content:\", response.text)\n",
    "                return None\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"An error occurred during the fetch file link request: {e}\")\n",
    "            return None\n",
    "        \n",
    "    def download_files_for_case(self, file_metadata, case_id, output_dir=\"cases\"):\n",
    "        \"\"\"\n",
    "        Download all files for a specific case and organize them into \n",
    "        data_category folders. Prepend the data_type to the filename to differentiate.\n",
    "\n",
    "        :param file_metadata: Required list of files associated with the given case_id.\n",
    "        :param data_types: Required case_id which files are associated with.\n",
    "        :param output_dir: Optional output directory, default is \"./cases\".\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        case_output_dir = os.path.join(output_dir, case_id)\n",
    "        \n",
    "        # creates directory for all files of current case_id\n",
    "        os.makedirs(case_output_dir, exist_ok=True)\n",
    "\n",
    "        for _, file_info in file_metadata.iterrows():\n",
    "            file_id = file_info['file_id']\n",
    "            file_name = file_info['file_name']\n",
    "            data_category = file_info['data_category']\n",
    "            data_type = file_info['data_type']\n",
    "        \n",
    "            # creates directory for all files in a particular data_category\n",
    "            category_folder = os.path.join(case_output_dir, data_category)\n",
    "            os.makedirs(category_folder, exist_ok=True)\n",
    "\n",
    "            file_path = os.path.join(category_folder, file_name)\n",
    "\n",
    "            # check if the file already exists, if so, skip the download\n",
    "            if os.path.exists(file_path):\n",
    "                print(f\"File {file_name} already exists, skipping download.\")\n",
    "                continue\n",
    "\n",
    "            # download the file using GDC API \n",
    "            # TODO: see if can be optimized, benchmark chunk_size, etc\n",
    "            url = f\"{self.GDC_BASE_URL}/data/{file_id}\"\n",
    "            response = requests.get(url, stream=True)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                with open(file_path, 'wb') as f:\n",
    "                    for chunk in response.iter_content(chunk_size=1024):\n",
    "                        if chunk:\n",
    "                            f.write(chunk)\n",
    "                    print(f\"Downloaded {file_name} for case: {case_id} in {data_category}/{file_name}\")\n",
    "            else:\n",
    "                print(f\"Failed to download {file_name} for case: {case_id}\")\n",
    "\n",
    "    def download_files(self, file_metadata, output_dir=\"cases\"):\n",
    "        \"\"\"\n",
    "        Download a specific set of files returned by fetch_file_links() and save it to the output directory, sorted by case_id and category.\n",
    "        Downloads in order of file_metadata, which is not sorted by case_id! Files will download out of order!\n",
    "        \n",
    "        :param file_id: The file_id of the file to be downloaded.\n",
    "        :param output_dir: Optional output directory, default is \"./cases\".\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        # creating base directory for the file\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        total_data = file_metadata[\"file_size\"].sum()\n",
    "\n",
    "        with tqdm(total=total_data, unit=\"B\", unit_scale=True, desc=\"Downloading files\", leave=False) as total_pb:\n",
    "            for _, file_info in file_metadata.iterrows():\n",
    "                file_id = file_info['file_id']\n",
    "                file_name = file_info['file_name']\n",
    "                \n",
    "                if len(file_info['cases']) > 1:\n",
    "                    case_id = \"GENERAL_METADATA\"\n",
    "                else:\n",
    "                    case_id = file_info['cases'][0]['case_id']\n",
    "\n",
    "                data_category = file_info['data_category']\n",
    "                file_size = file_info['file_size']\n",
    "\n",
    "                case_output_dir = os.path.join(output_dir, case_id)\n",
    "                os.makedirs(case_output_dir, exist_ok=True)\n",
    "\n",
    "                category_folder = os.path.join(case_output_dir, data_category)\n",
    "                os.makedirs(category_folder, exist_ok=True)\n",
    "\n",
    "                file_path = os.path.join(category_folder, file_name)\n",
    "\n",
    "                if os.path.exists(file_path):\n",
    "                    # print(f\"File {file_name} for case {case_id} already exists, skipping download.\")\n",
    "                    total_pb.update(file_size)\n",
    "                    continue\n",
    "\n",
    "                temp_file_path = file_path + \".part\"\n",
    "\n",
    "                if os.path.exists(temp_file_path):\n",
    "                    print(f\"Partially downloaded file {temp_file_path} found. Removing it.\")\n",
    "                    os.remove(temp_file_path)\n",
    "\n",
    "                url = f\"{self.GDC_BASE_URL}/data/{file_id}\"\n",
    "                response = requests.get(url, stream=True)\n",
    "                if response.status_code == 200:\n",
    "                    with tqdm(total=file_size, unit=\"B\", unit_scale=True, desc=f\"Downloading {file_name}\", leave=False) as file_pb:\n",
    "                        with open(temp_file_path, 'wb') as f:\n",
    "                            for chunk in response.iter_content(chunk_size=1024):\n",
    "                                if chunk:\n",
    "                                    f.write(chunk)\n",
    "                                    file_pb.update(len(chunk))\n",
    "                                    total_pb.update(len(chunk))\n",
    "                        file_pb.close()\n",
    "                    os.rename(temp_file_path, file_path)\n",
    "                    # print(f\"Downloaded {file_name} for case: {case_id} in {data_category}/{file_name}\")\n",
    "                else:\n",
    "                    print(f\"Failed to download {file_name} for case: {case_id}\")\n",
    "    \n",
    "    async def download_files_async(self, file_metadata, output_dir=\"cases\"):\n",
    "        \"\"\"\n",
    "        Asynchronously download files from GDC using file metadata.\n",
    "\n",
    "        :param file_metadata: DataFrame containing file metadata with columns 'file_id', 'file_name', 'cases', 'data_category', and 'file_size'.\n",
    "        :param output_dir: Directory where files will be saved, defaults to \"cases\".\n",
    "        \"\"\"\n",
    "\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        total_size = file_metadata[\"file_size\"].sum()\n",
    "\n",
    "        async def download_file(session, file_info, progress_bar):\n",
    "            file_id = file_info[\"file_id\"]\n",
    "            file_name = file_info[\"file_name\"]\n",
    "            case_id = \"GENERAL_METADATA\" if len(file_info[\"cases\"]) > 1 else file_info[\"cases\"][0][\"case_id\"]\n",
    "            data_category = file_info[\"data_category\"]\n",
    "            file_size = file_info[\"file_size\"]\n",
    "\n",
    "            case_dir = os.path.join(output_dir, case_id)\n",
    "            os.makedirs(case_dir, exist_ok=True)\n",
    "            category_dir = os.path.join(case_dir, data_category)\n",
    "            os.makedirs(category_dir, exist_ok=True)\n",
    "\n",
    "            temp_file_path = os.path.join(category_dir, f\"{file_name}.part\")\n",
    "            final_file_path = os.path.join(category_dir, file_name)\n",
    "            \n",
    "            if os.path.exists(final_file_path):\n",
    "                #print(\"file exists already\")\n",
    "                progress_bar.update(file_size)\n",
    "                progress_bar.refresh()\n",
    "                return\n",
    "            elif os.path.exists(temp_file_path):\n",
    "                os.remove(temp_file_path)\n",
    "\n",
    "            url = f\"{self.GDC_BASE_URL}/data/{file_id}\"\n",
    "\n",
    "            try:\n",
    "                async with session.get(url) as response:\n",
    "                    if response.status == 200:\n",
    "                        with open(temp_file_path, 'wb') as f:\n",
    "                            total_downloaded = 0\n",
    "                            async for chunk in response.content.iter_any():\n",
    "                                if chunk:\n",
    "                                    f.write(chunk)\n",
    "                                    total_downloaded += len(chunk)\n",
    "                                    progress_bar.update(len(chunk))\n",
    "                                    progress_bar.set_postfix({\"Downloaded\": f\"{total_downloaded}/{file_size}\"})\n",
    "                                    progress_bar.refresh()\n",
    "                                    \n",
    "                        os.rename(temp_file_path, final_file_path)\n",
    "                    else:\n",
    "                        print(f\"Failed to download {file_name}: {response.status}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error downloading {file_name}: {e}\")\n",
    "\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            with tqdm_async(total=total_size, unit=\"B\", unit_scale=True, desc=f\"Downloading files\", leave=False) as progress_bar:\n",
    "                tasks = [\n",
    "                    download_file(session, file_info, progress_bar)\n",
    "                    for _, file_info in file_metadata.iterrows()\n",
    "                ]\n",
    "                await asyncio.gather(*tasks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 cases.\n"
     ]
    }
   ],
   "source": [
    "gdc_fetcher = GDCDataFetcher(project_id=\"TCGA-COAD\")\n",
    "\n",
    "cases = gdc_fetcher.fetch_cases()\n",
    "cases = cases[:10]\n",
    "\n",
    "# data_categories = [\n",
    "#     \"Clinical\", \n",
    "#     \"Biospecimen\", \n",
    "#     \"DNA Methylation\", \n",
    "#     \"Copy Number Variation\", \n",
    "#     \"Simple Nucleotide Variation\", \n",
    "#     \"Transcriptome Profiling\",\n",
    "#     \"Sequencing Reads\"\n",
    "# ]\n",
    "\n",
    "data_categories = [\"Clinical\", \"Biospecimen\", \"DNA Methylation\"]\n",
    "\n",
    "print(\"Found\", cases[\"id\"].size, \"cases.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 137 files with 6.390 GB.\n"
     ]
    }
   ],
   "source": [
    "os.system(\"\")\n",
    "\n",
    "case_metadata = {}\n",
    "total_cases = len(cases)\n",
    "\n",
    "case_ids = [case.id for case in cases.itertuples()]\n",
    "\n",
    "# Fetch metadata for all case IDs in a single query\n",
    "file_metadata = gdc_fetcher.fetch_file_links(case_ids, data_categories=data_categories)\n",
    "\n",
    "if file_metadata is not None:\n",
    "    total_files = file_metadata.shape[0]\n",
    "    total_size_GB = file_metadata[\"file_size\"].sum() / 2**30\n",
    "    print(f\"Found {total_files} files with {total_size_GB:.3f} GB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    }
   ],
   "source": [
    "#gdc_fetcher.download_files(file_metadata, \"cases_TEST_nonasync\")\n",
    "await gdc_fetcher.download_files_async(file_metadata, \"cases_TEST_TRAIN_100\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
